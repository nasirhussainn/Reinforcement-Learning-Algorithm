{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68ee468",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2 style=\"color:blue;font-size:30px;\">Artificial Intelligence CS-414</h2>\n",
    "<h3 style=\"color:purple\">Assignment 4</h3>\n",
    " </center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f970a6",
   "metadata": {},
   "source": [
    "<p>Consider Volcano crossing problem discussed in the class. Consider an instance of the problem using different grid size, rewards of end states etc. You are free to define the problem, its states, actions etc. Solve the problem using the following algorithms.</p>\n",
    "<ul style=\"color:purple\">\n",
    "<li>Model free Monte Carlo</li>\n",
    "<li>SARSA</li>\n",
    "<li>Q-Learning</li>\n",
    "</ul>\n",
    "<ol style=\"color:green\">\n",
    "<li>Run the algorithms using different number of episodes of uniformly random policy and show Q-values and average utility.</li>\n",
    "<li>Use different slip probabilities ranging from 0.0 to 0.3 and show your results on different algorithms.</li>\n",
    "<li>Use epsilon greedy algorithms to change generate episode from uniformly random policy for exploration as well as policy that chooses the best action.</li>\n",
    "<li>Write a 2-3 page report and explain your code and results in it.</li>\n",
    "Develop a GUI based user friendly application from which user to choose appropriate options e.g slip probability, epsilon value, no of episodes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae01ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d536303",
   "metadata": {},
   "source": [
    "<h3 style=\"color:purple\">MODEL FREE MONTE CARLO</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80350e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob):\n",
    "    # Initialize the grid world\n",
    "    grid = np.zeros(grid_size)\n",
    "    \n",
    "    # Set rewards for safe end states\n",
    "    for state in safe_end_states:\n",
    "        if 0 <= state[0] < grid_size[0] and 0 <= state[1] < grid_size[1]:\n",
    "            grid[state[0], state[1]] = 20\n",
    "\n",
    "    # Set rewards for dangerous end states\n",
    "    for state in dangerous_end_state:\n",
    "        if 0 <= state[0] < grid_size[0] and 0 <= state[1] < grid_size[1]:\n",
    "            grid[state[0], state[1]] = -50\n",
    "\n",
    "    # Set reward for the specific cell (2, 0)\n",
    "    specific_reward_location = (2, 0)\n",
    "    if 0 <= specific_reward_location[0] < grid_size[0] and 0 <= specific_reward_location[1] < grid_size[1]:\n",
    "        grid[specific_reward_location[0], specific_reward_location[1]] = 2\n",
    "    \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b3157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the next state based on the action and slip probability\n",
    "def get_next_state(current_state, action, slip_prob, grid):\n",
    "    # Define possible directions\n",
    "    directions = {\"N\": (-1, 0), \"E\": (0, 1), \"S\": (1, 0), \"W\": (0, -1)}\n",
    "    \n",
    "    # Convert integer action index to string\n",
    "    action_str = list(directions.keys())[action]\n",
    "    \n",
    "    # Check if slip occurs based on slip probability\n",
    "    if np.random.rand() < slip_prob:\n",
    "        # Slip: Move to a random adjacent state\n",
    "        possible_actions = list(directions.keys())\n",
    "        possible_actions.remove(action_str)  # Remove the current action to avoid going back\n",
    "        random_action = np.random.choice(possible_actions)\n",
    "        next_state = tuple(np.array(current_state) + np.array(directions[random_action]))\n",
    "    else:\n",
    "        # No slip: Move in the chosen direction\n",
    "        next_state = tuple(np.array(current_state) + np.array(directions[action_str]))\n",
    "    \n",
    "    # Ensure the next state is within the grid\n",
    "    next_state = (\n",
    "        max(0, min(next_state[0], grid.shape[0] - 1)),\n",
    "        max(0, min(next_state[1], grid.shape[1] - 1))\n",
    "    )\n",
    "    \n",
    "    return next_state\n",
    "\n",
    "# Function to get the reward for a given state\n",
    "def get_reward(state, grid):\n",
    "    # Return the reward for the given state from the grid\n",
    "    return grid[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbeb57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform epsilon-greedy action selection\n",
    "def epsilon_greedy(Q, state, epsilon, num_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Exploration: Choose a random action\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        # Exploitation: Choose the action with the highest Q-value\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad8b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Monte Carlo sampling with epsilon-greedy exploration\n",
    "def monte_carlo(grid, slip_prob, num_episodes, epsilon):\n",
    "    # Initialize Q-values\n",
    "    Q = np.zeros_like(grid)\n",
    "    returns = np.zeros_like(grid)\n",
    "    visit_count = np.zeros_like(grid)\n",
    "    \n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize the episode\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "    \n",
    "        # Starting state\n",
    "        current_state = (2, 1)\n",
    "    \n",
    "        # Generate an episode using an epsilon-greedy policy\n",
    "        while True:\n",
    "            # Choose action using epsilon-greedy strategy\n",
    "            num_actions = len(Q)  # Total number of possible actions (assuming each state has the same number of actions)\n",
    "            action = epsilon_greedy(Q, current_state, epsilon, num_actions)\n",
    "        \n",
    "            # Store current state and action\n",
    "            episode_states.append(current_state)\n",
    "            episode_actions.append(action)\n",
    "        \n",
    "            # Determine the next state based on the action and slip probability\n",
    "            next_state = get_next_state(current_state, action, slip_prob, grid)\n",
    "        \n",
    "            # Get the reward for the next state\n",
    "            reward = get_reward(next_state, grid)\n",
    "            episode_rewards.append(reward)\n",
    "        \n",
    "            # Update the current state\n",
    "            current_state = next_state\n",
    "        \n",
    "            # Check if the episode has ended\n",
    "            if next_state in [(0, 3), (2, 3)]:\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "        # Update Q-values based on the observed returns\n",
    "        total_return = 0\n",
    "        for t in range(len(episode_states) - 1, -1, -1):\n",
    "            total_return += episode_rewards[t]\n",
    "            \n",
    "            # If the state is not visited before in this episode\n",
    "            if episode_states[t] not in episode_states[:t]:\n",
    "                state = episode_states[t]\n",
    "                action = episode_actions[t]\n",
    "                \n",
    "                # Increment visit count for the state-action pair\n",
    "                visit_count[state] += 1\n",
    "                \n",
    "                # Update Q-value using an incremental formula\n",
    "                Q[state] += (total_return - Q[state]) / visit_count[state]\n",
    "                \n",
    "    # Calculate average utility\n",
    "    average_utility = np.mean(Q)\n",
    "    \n",
    "    return Q, average_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b329bca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Slip Probability 0.0 and Epsilon 0.1:\n",
      "Q-values:\n",
      "[[    0.         -1481.45766345 -1406.34441088     0.        ]\n",
      " [    0.         -1466.59772492 -1421.62921348     0.        ]\n",
      " [    0.         -1451.95       -1372.46376812     0.        ]]\n",
      "Average Utility: -716.7035650707409\n",
      "\n",
      "\n",
      "Results for Slip Probability 0.1 and Epsilon 0.1:\n",
      "Q-values:\n",
      "[[-8.33034325e+02 -8.27196903e+02 -7.76281377e+02  0.00000000e+00]\n",
      " [-8.51178683e+02 -8.27028815e+02 -7.73512438e+02 -6.57894737e-01]\n",
      " [-8.11696203e+02 -8.26316000e+02 -8.19299270e+02  0.00000000e+00]]\n",
      "Average Utility: -612.1834923351734\n",
      "\n",
      "\n",
      "Results for Slip Probability 0.2 and Epsilon 0.1:\n",
      "Q-values:\n",
      "[[-602.52319109 -575.66481687 -523.80246914    0.        ]\n",
      " [-592.86474501 -572.624052   -525.75379939  -40.25423729]\n",
      " [-567.24293785 -570.798      -558.04          0.        ]]\n",
      "Average Utility: -427.4640207207772\n",
      "\n",
      "\n",
      "Results for Slip Probability 0.3 and Epsilon 0.1:\n",
      "Q-values:\n",
      "[[-409.94127243 -410.1754386  -363.68421053    0.        ]\n",
      " [-416.78638941 -405.69125993 -348.12536443  -64.93478261]\n",
      " [-435.63194444 -403.286      -370.7440273     0.        ]]\n",
      "Average Utility: -302.41672414064504\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to run experiments with different slip probabilities and epsilon values\n",
    "def run_experiments():\n",
    "    grid_size = (3, 4)\n",
    "    start_state = (0, 0)\n",
    "    safe_end_states = [(3, 1), (1, 4)]  # Valid column indices are 0, 1, 2, 3\n",
    "    dangerous_end_state = [(0, 2),(1,2)]  # Assuming this is meant to be a dangerous state\n",
    "    num_episodes = 1000  # Adjust as needed\n",
    "    \n",
    "    slip_probabilities = [0.0, 0.1, 0.2, 0.3]\n",
    "    epsilon_values = [0.1]  # Adjust as needed\n",
    "    \n",
    "    for slip_prob in slip_probabilities:\n",
    "        for epsilon in epsilon_values:\n",
    "            # Initialize the environment\n",
    "            grid = initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob)\n",
    "            \n",
    "            # Run Monte Carlo algorithm with epsilon-greedy exploration\n",
    "            Q_values, avg_utility = monte_carlo(grid, slip_prob, num_episodes, epsilon)\n",
    "            \n",
    "            # Display results for each slip probability and epsilon value\n",
    "            print(f\"Results for Slip Probability {slip_prob} and Epsilon {epsilon}:\")\n",
    "            print(\"Q-values:\")\n",
    "            print(Q_values)\n",
    "            print(f\"Average Utility: {avg_utility}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Main function to run experiments\n",
    "def main():\n",
    "    run_experiments()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d3d6e",
   "metadata": {},
   "source": [
    "<h3 style=\"color:purple\">SARSA</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba15f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform SARSA with epsilon-greedy exploration\n",
    "def sarsa(grid, slip_prob, num_episodes, epsilon, alpha, gamma):\n",
    "    # Initialize Q-values\n",
    "    Q = np.zeros(grid.shape + (4,))  # Separate Q array for each state-action pair\n",
    "    \n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Starting state\n",
    "        current_state = (2, 1)\n",
    "        \n",
    "        # Choose action using epsilon-greedy strategy\n",
    "        num_actions = Q.shape[2]\n",
    "        current_action = epsilon_greedy(Q, current_state, epsilon, num_actions)\n",
    "        \n",
    "        # Initialize flag for episode completion\n",
    "        episode_complete = False\n",
    "        \n",
    "        while not episode_complete:\n",
    "            # Determine the next state based on the action and slip probability\n",
    "            next_state = get_next_state(current_state, current_action, slip_prob, grid)\n",
    "            \n",
    "            # Get the reward for the next state\n",
    "            reward = get_reward(next_state, grid)\n",
    "            \n",
    "            # Choose next action using epsilon-greedy strategy\n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon, num_actions)\n",
    "            \n",
    "            # Update Q-value using the SARSA update rule\n",
    "            Q[current_state + (current_action,)] += alpha * (reward + gamma * Q[next_state + (next_action,)] - Q[current_state + (current_action,)])\n",
    "            \n",
    "            # Update current state and action\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "            \n",
    "            # Check if the episode has ended\n",
    "            episode_complete = next_state in [(0, 3), (2, 3)]\n",
    "    \n",
    "    # Calculate average utility\n",
    "    average_utility = np.mean(Q)\n",
    "    \n",
    "    return Q, average_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6fb03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to run experiments with different slip probabilities, epsilon values, alpha, and gamma\n",
    "def run_sarsa_experiments():\n",
    "   \n",
    "    grid_size = (3, 4)\n",
    "    start_state = (0, 0)\n",
    "    safe_end_states = [(3, 1), (1, 4)]  # Valid column indices are 0, 1, 2, 3\n",
    "    dangerous_end_state = [(0, 2),(1,2)]  # Assuming this is meant to be a dangerous state\n",
    "    num_episodes = 1000  # Adjust as needed\n",
    "    \n",
    "    slip_probabilities = [0.0, 0.1, 0.2, 0.3]\n",
    "    epsilon_values = [0.1]  # Adjust as needed\n",
    "    alpha_values = [0.1]  # Learning rate\n",
    "    gamma_values = [0.9]  # Discount factor\n",
    "    \n",
    "    for slip_prob in slip_probabilities:\n",
    "        for epsilon in epsilon_values:\n",
    "            for alpha in alpha_values:\n",
    "                for gamma in gamma_values:\n",
    "                    # Initialize the environment\n",
    "                    grid = initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob)\n",
    "                    \n",
    "                    # Run SARSA algorithm with epsilon-greedy exploration\n",
    "                    Q_values, avg_utility = sarsa(grid, slip_prob, num_episodes, epsilon, alpha, gamma)\n",
    "                    \n",
    "                    # Display results for each combination of parameters\n",
    "                    print(f\"Results for Slip Probability {slip_prob}, Epsilon {epsilon}, Alpha {alpha}, Gamma {gamma}:\")\n",
    "                    print(\"Q-values:\")\n",
    "                    print(Q_values)\n",
    "                    print(f\"Average Utility: {avg_utility}\")\n",
    "                    print(\"\\n\")\n",
    "\n",
    "# Main function to run SARSA experiments\n",
    "def main_sarsa():\n",
    "    run_sarsa_experiments()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_sarsa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ce641",
   "metadata": {},
   "source": [
    "<h3 style=\"color:purple\">Q LEARNING</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f5813d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Q-learning with epsilon-greedy exploration\n",
    "def q_learning(grid, slip_prob, num_episodes, epsilon, alpha, gamma):\n",
    "    # Initialize Q-values\n",
    "    Q = np.zeros(grid.shape + (4,))  # Separate Q array for each state-action pair\n",
    "    \n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Starting state\n",
    "        current_state = (2, 1)\n",
    "        \n",
    "        # Initialize flag for episode completion\n",
    "        episode_complete = False\n",
    "        \n",
    "        while not episode_complete:\n",
    "            # Choose action using epsilon-greedy strategy\n",
    "            num_actions = Q.shape[2]\n",
    "            current_action = epsilon_greedy(Q, current_state, epsilon, num_actions)\n",
    "            \n",
    "            # Determine the next state based on the action and slip probability\n",
    "            next_state = get_next_state(current_state, current_action, slip_prob, grid)\n",
    "            \n",
    "            # Get the reward for the next state\n",
    "            reward = get_reward(next_state, grid)\n",
    "            \n",
    "            # Update Q-value using the Q-learning update rule\n",
    "            Q[current_state + (current_action,)] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[current_state + (current_action,)])\n",
    "            \n",
    "            # Update current state\n",
    "            current_state = next_state\n",
    "            \n",
    "            # Check if the episode has ended\n",
    "            episode_complete = next_state in [(0, 3), (2, 3)]\n",
    "    \n",
    "    # Calculate average utility\n",
    "    average_utility = np.mean(Q)\n",
    "    \n",
    "    return Q, average_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments with different slip probabilities, epsilon values, alpha, and gamma\n",
    "def run_q_learning_experiments():\n",
    "    grid_size = (3, 4)\n",
    "    start_state = (0, 0)\n",
    "    safe_end_states = [(3, 1), (1, 4)]  # Valid column indices are 0, 1, 2, 3\n",
    "    dangerous_end_state = [(0, 2),(1,2)]  # Assuming this is meant to be a dangerous state\n",
    "    num_episodes = 1000  # Adjust as needed\n",
    "    \n",
    "    slip_probabilities = [0.0, 0.1, 0.2, 0.3]\n",
    "    epsilon_values = [0.1]  # Adjust as needed\n",
    "    alpha_values = [0.1]  # Learning rate\n",
    "    gamma_values = [0.9]  # Discount factor\n",
    "    \n",
    "    for slip_prob in slip_probabilities:\n",
    "        for epsilon in epsilon_values:\n",
    "            for alpha in alpha_values:\n",
    "                for gamma in gamma_values:\n",
    "                    # Initialize the environment\n",
    "                    grid = initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob)\n",
    "                    \n",
    "                    # Run Q-learning algorithm with epsilon-greedy exploration\n",
    "                    Q_values, avg_utility = q_learning(grid, slip_prob, num_episodes, epsilon, alpha, gamma)\n",
    "                    \n",
    "                    # Display results for each combination of parameters\n",
    "                    print(f\"Results for Slip Probability {slip_prob}, Epsilon {epsilon}, Alpha {alpha}, Gamma {gamma}:\")\n",
    "                    print(\"Q-values:\")\n",
    "                    print(Q_values)\n",
    "                    print(f\"Average Utility: {avg_utility}\")\n",
    "                    print(\"\\n\")\n",
    "\n",
    "# Main function to run Q-learning experiments\n",
    "def main_q_learning():\n",
    "    run_q_learning_experiments()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_q_learning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb961ee",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3 style=\"color:purple\">GUI</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821eacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments with different slip probabilities and epsilon values\n",
    "def run_experiments(slip_prob, epsilon, num_episodes):\n",
    "    grid_size = (4, 4)\n",
    "    start_state = (2, 1)\n",
    "    safe_end_states = [(0, 3), (2, 3)]  # Valid column indices are 0, 1, 2, 3\n",
    "    dangerous_end_state = (2, 3)  # Assuming this is meant to be a dangerous state\n",
    "    \n",
    "    if not isinstance(slip_prob, (list, tuple)):\n",
    "        slip_prob = [slip_prob]\n",
    "    \n",
    "    if not isinstance(epsilon, (list, tuple)):\n",
    "        epsilon = [epsilon]\n",
    "    \n",
    "    for slip_prob_val in slip_prob:\n",
    "        for epsilon_val in epsilon:\n",
    "            # Initialize the environment\n",
    "            grid = initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob_val)\n",
    "            \n",
    "            # Run Monte Carlo algorithm with epsilon-greedy exploration\n",
    "            Q_values, avg_utility = monte_carlo(grid, slip_prob_val, num_episodes, epsilon_val)\n",
    "            \n",
    "            # Display results for each slip probability and epsilon value\n",
    "            print(\"MONTO CARLO FREE MODEL\")\n",
    "            print(f\"Results for Slip Probability {slip_prob_val} and Epsilon {epsilon_val}:\")\n",
    "            print(\"Q-values:\")\n",
    "            print(Q_values)\n",
    "            print(f\"Average Utility: {avg_utility}\")\n",
    "            print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0252d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments with different slip probabilities, epsilon values, alpha, and gamma\n",
    "def run_sarsa_experiments(slip_probabilities, epsilon_values, num_episodes):\n",
    "    grid_size = (4, 4)\n",
    "    start_state = (2, 1)\n",
    "    safe_end_states = [(0, 3), (2, 3)]  # Valid column indices are 0, 1, 2, 3\n",
    "    dangerous_end_state = (2, 3)  # Assuming this is meant to be a dangerous state\n",
    "    alpha_values = [0.1]  # Learning rate\n",
    "    gamma_values = [0.9]  # Discount factor\n",
    "    \n",
    "    if not isinstance(slip_probabilities, (list, tuple)):\n",
    "        slip_probabilities = [slip_probabilities]\n",
    "    \n",
    "    if not isinstance(epsilon_values, (list, tuple)):\n",
    "        epsilon_values = [epsilon_values]\n",
    "    \n",
    "    if not isinstance(alpha_values, (list, tuple)):\n",
    "        alpha_values = [alpha_values]\n",
    "    \n",
    "    if not isinstance(gamma_values, (list, tuple)):\n",
    "        gamma_values = [gamma_values]\n",
    "    \n",
    "    for slip_prob in slip_probabilities:\n",
    "        for epsilon in epsilon_values:\n",
    "            for alpha in alpha_values:\n",
    "                for gamma in gamma_values:\n",
    "                    # Initialize the environment\n",
    "                    grid = initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob)\n",
    "                    \n",
    "                    # Run SARSA algorithm with epsilon-greedy exploration\n",
    "                    Q_values, avg_utility = sarsa(grid, slip_prob, num_episodes, epsilon, alpha, gamma)\n",
    "                    \n",
    "                    # Display results for each combination of parameters\n",
    "                    print(\"SARSA\")\n",
    "                    print(f\"Results for Slip Probability {slip_prob}, Epsilon {epsilon}, Alpha {alpha}, Gamma {gamma}:\")\n",
    "                    print(\"Q-values:\")\n",
    "                    print(Q_values)\n",
    "                    print(f\"Average Utility: {avg_utility}\")\n",
    "                    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd2ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run experiments with different slip probabilities, epsilon values, alpha, and gamma\n",
    "def run_q_learning_experiments(slip_probabilities, epsilon_values, num_episodes):\n",
    "    grid_size = (4, 4)\n",
    "    start_state = (2, 1)\n",
    "    safe_end_states = [(0, 3), (2, 3)]  # Valid column indices are 0, 1, 2, 3\n",
    "    dangerous_end_state = (2, 3)  # Assuming this is meant to be a dangerous state\n",
    "    \n",
    "    alpha_values = [0.1]  # Learning rate\n",
    "    gamma_values = [0.9]  # Discount factor\n",
    "    \n",
    "    if not isinstance(slip_probabilities, (list, tuple)):\n",
    "        slip_probabilities = [slip_probabilities]\n",
    "    \n",
    "    if not isinstance(epsilon_values, (list, tuple)):\n",
    "        epsilon_values = [epsilon_values]\n",
    "    \n",
    "    if not isinstance(alpha_values, (list, tuple)):\n",
    "        alpha_values = [alpha_values]\n",
    "    \n",
    "    if not isinstance(gamma_values, (list, tuple)):\n",
    "        gamma_values = [gamma_values]\n",
    "    \n",
    "    for slip_prob in slip_probabilities:\n",
    "        for epsilon in epsilon_values:\n",
    "            for alpha in alpha_values:\n",
    "                for gamma in gamma_values:\n",
    "                    # Initialize the environment\n",
    "                    grid = initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob)\n",
    "                    \n",
    "                    # Run Q-learning algorithm with epsilon-greedy exploration\n",
    "                    Q_values, avg_utility = q_learning(grid, slip_prob, num_episodes, epsilon, alpha, gamma)\n",
    "                    \n",
    "                    # Display results for each combination of parameters\n",
    "                    print(\"Q LEARNING\")\n",
    "                    print(f\"Results for Slip Probability {slip_prob}, Epsilon {epsilon}, Alpha {alpha}, Gamma {gamma}:\")\n",
    "                    print(\"Q-values:\")\n",
    "                    print(Q_values)\n",
    "                    print(f\"Average Utility: {avg_utility}\")\n",
    "                    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fcd5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_grid(canvas, grid):\n",
    "    cell_size = 30\n",
    "    canvas.delete(\"all\")\n",
    "\n",
    "    for row_index, row in enumerate(grid):\n",
    "        for col_index, value in enumerate(row):\n",
    "            x1 = col_index * cell_size\n",
    "            y1 = row_index * cell_size\n",
    "            x2 = x1 + cell_size\n",
    "            y2 = y1 + cell_size\n",
    "\n",
    "            canvas.create_rectangle(x1, y1, x2, y2, outline=\"black\", fill=\"white\")\n",
    "            canvas.create_text((x1 + x2) / 2, (y1 + y2) / 2, text=str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1beb99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTO CARLO FREE MODEL\n",
      "Results for Slip Probability 0.1 and Epsilon 0.1:\n",
      "Q-values:\n",
      "[[19.68325792 19.55696203 19.57273652  0.        ]\n",
      " [19.6146789  19.55602537 19.29054054 18.02816901]\n",
      " [18.73493976 19.23       15.19083969  0.        ]\n",
      " [14.61538462 20.         20.          0.        ]]\n",
      "Average Utility: 15.192095897342918\n",
      "\n",
      "\n",
      "SARSA\n",
      "Results for Slip Probability 0.1, Epsilon 0.1, Alpha 0.1, Gamma 0.9:\n",
      "Q-values:\n",
      "[[[ 2.45008691 14.27648713  1.42257611  1.21461066]\n",
      "  [14.42623998 16.84172613 12.0583313   9.69478107]\n",
      "  [13.72896288 19.99159362 11.23063682 14.1135241 ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 4.31715696 12.93204497  2.81160009  3.59003026]\n",
      "  [15.13647975 11.33794312  9.95238185  9.7417079 ]\n",
      "  [14.70048644  0.18327686  1.86548976  7.12606218]\n",
      "  [ 6.90454261  0.          0.          0.        ]]\n",
      "\n",
      " [[ 9.94079184  2.08883492  0.7594411   2.50957564]\n",
      "  [12.58315606  5.78178391  7.24674804  5.95602533]\n",
      "  [ 2.65718016  0.10690403  0.38118101 10.39679769]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 3.56269229  0.          0.          0.        ]\n",
      "  [ 9.93935394  0.52587839  0.          0.        ]\n",
      "  [ 4.12884806  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "Average Utility: 4.853342997939535\n",
      "\n",
      "\n",
      "Q LEARNING\n",
      "Results for Slip Probability 0.1, Epsilon 0.1, Alpha 0.1, Gamma 0.9:\n",
      "Q-values:\n",
      "[[[ 5.28646274e+00  1.52308198e+01  2.72458040e+00  1.85523240e+00]\n",
      "  [ 1.45289756e+01  1.73114082e+01  1.29182943e+01  1.35211599e+01]\n",
      "  [ 1.75344600e+01  1.97612298e+01  1.19707247e+01  1.56414264e+01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.37100387e+01  2.43836465e+00  2.17362047e+00  7.14546080e-01]\n",
      "  [ 1.52728289e+01  1.21506412e+01  1.20182689e+01  1.13300183e+01]\n",
      "  [ 8.11026603e+00  9.90241309e-02  1.61224930e+00  1.37651578e+01]\n",
      "  [-5.00000000e+00 -4.40097587e+00  0.00000000e+00  2.26383681e+00]]\n",
      "\n",
      " [[ 2.22592315e-02  1.11003566e+01  4.47754780e-01  0.00000000e+00]\n",
      "  [ 1.32843789e+01  9.36971389e+00  8.77864559e+00  9.63568151e+00]\n",
      "  [ 7.01430180e+00  0.00000000e+00  1.72756768e-01  2.99943646e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 5.16295849e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 1.16340369e+01  2.53821948e-01  1.08986930e+00  3.56342435e-01]\n",
      "  [ 4.92303553e+00  1.16120142e-02  0.00000000e+00  0.00000000e+00]\n",
      "  [-5.00000000e+00  0.00000000e+00  0.00000000e+00  2.43981321e-01]]]\n",
      "Average Utility: 4.938181303358396\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from functools import partial\n",
    "def on_run_button_click(canvas, slip_prob_entry, epsilon_entry, num_episodes_entry):\n",
    "    slip_prob = float(slip_prob_entry.get())\n",
    "    epsilon = float(epsilon_entry.get())\n",
    "    num_episodes = int(num_episodes_entry.get())\n",
    "    \n",
    "    grid_size = (4, 4)\n",
    "    start_state = (2, 1)\n",
    "    safe_end_states = [(0, 3), (2, 3)]\n",
    "    dangerous_end_state = (2, 3)\n",
    "    grid = initialize_environment(grid_size, start_state, safe_end_states, dangerous_end_state, slip_prob)\n",
    "    display_grid(canvas, grid)\n",
    "\n",
    "    run_experiments(slip_prob, epsilon, num_episodes)\n",
    "    run_sarsa_experiments(slip_prob, epsilon, num_episodes)\n",
    "    run_q_learning_experiments(slip_prob, epsilon, num_episodes)\n",
    "\n",
    "\n",
    "def create_gui():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Reinforcement Learning Experiments\")\n",
    "\n",
    "    # Create labels and entry fields\n",
    "    tk.Label(root, text=\"Slip Probability:\").grid(row=0, column=0, padx=10, pady=5)\n",
    "    slip_prob_entry = tk.Entry(root)\n",
    "    slip_prob_entry.grid(row=0, column=1, padx=10, pady=5)\n",
    "\n",
    "    tk.Label(root, text=\"Epsilon Value:\").grid(row=1, column=0, padx=10, pady=5)\n",
    "    epsilon_entry = tk.Entry(root)\n",
    "    epsilon_entry.grid(row=1, column=1, padx=10, pady=5)\n",
    "\n",
    "    tk.Label(root, text=\"Number of Episodes:\").grid(row=2, column=0, padx=10, pady=5)\n",
    "    num_episodes_entry = tk.Entry(root)\n",
    "    num_episodes_entry.grid(row=2, column=1, padx=10, pady=5)\n",
    "\n",
    "    canvas = tk.Canvas(root, width=120, height=120)\n",
    "    canvas.grid(row=4, column=0, columnspan=2, pady=10)\n",
    "\n",
    "    run_button = tk.Button(root, text=\"Run Experiments\", command=partial(on_run_button_click, canvas, slip_prob_entry, epsilon_entry, num_episodes_entry))\n",
    "    run_button.grid(row=3, column=0, columnspan=2, pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "    \n",
    "create_gui()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cadfad9",
   "metadata": {},
   "source": [
    "<h3 style=\"color:purple\">Submitted By</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1ee8b",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li> Nasir Hussain</li>\n",
    "<li> Laiba Masood</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbfbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119fbf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
